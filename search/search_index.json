{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"downstream","text":"<p>downstream provides efficient, constant-space implementations of stream curation algorithms.</p> <ul> <li>Free software: MIT license</li> <li>Documentation: https://mmore500.github.io/downstream</li> </ul>"},{"location":"#ci-status","title":"CI Status","text":""},{"location":"#installation","title":"Installation","text":"<p>To install from PyPi with pip, run</p> <p><code>python3 -m pip install \"downstream[jit]\"</code></p> <p>A containerized release of <code>downstream</code> is available via https://ghcr.io</p> <pre><code>singularity exec docker://ghcr.io/mmore500/downstream python3 -m downstream --help\n</code></pre>"},{"location":"#quickstart-guide","title":"Quickstart Guide","text":"<p>https://mmore500.com/downstream/quickstart/</p>"},{"location":"#citing","title":"Citing","text":"<p>If downstream contributes to a scientific publication, please cite it as</p> <p>Yang C., Wagner J., Dolson E., Zaman L., &amp; Moreno M. A. (2025). Downstream: efficient cross-platform algorithms for fixed-capacity stream downsampling. arXiv preprint arXiv:2506.12975. https://doi.org/10.48550/arXiv.2506.12975</p> <pre><code>@misc{yang2025downstream,\n      doi={10.48550/arXiv.2506.12975},\n      url={https://arxiv.org/abs/2506.12975},\n      title={Downstream: efficient cross-platform algorithms for fixed-capacity stream downsampling},\n      author={Connor Yang and Joey Wagner and Emily Dolson and Luis Zaman and Matthew Andres Moreno},\n      year={2025},\n      eprint={2506.12975},\n      archivePrefix={arXiv},\n      primaryClass={cs.DS},\n}\n</code></pre> <p>And don't forget to leave a star on GitHub!</p>"},{"location":"#credits","title":"Credits","text":"<p>This package was created with Cookiecutter and the mmore500/cookiecutter-dishtiny-project project template.</p>"},{"location":"algorithm/","title":"Algorithm Selection","text":"<p>Downstream offers three core site selection algorithms. Each one curates a different temporal distribution of stored items while preserving the constant-space property of a traditional ring buffer.</p> Algorithm Distribution Typical use case Steady Uniform spacing across the entire stream Trend analysis or when every time period matters Stretched Density thins with depth in the stream Emphasize initial conditions or ancient history Tilted Density thins with item age Recent-history monitoring <p>Hybrid variants combine multiple algorithms by partitioning the buffer.</p> <p>All algorithms share the same interface. However, naive stretched and tilted algorithms have ingest limits of up to <code>2**S - 2</code> items, where <code>S</code> is the surface size. You can test limits programmatically using <code>*_algo.has_ingest_capacity</code> or <code>*_algo.get_ingest_capacity</code>.</p> <p>For more details on algorithm behavior and guarantees, see our paper on the underlying algorithms. For hstrat users, analysis of how algorithm choice affects phylogeny reconstruction is available here.</p>"},{"location":"citing/","title":"Citing","text":"<p>If downstream contributes to a scientific publication, please cite it as</p> <p>Yang C., Wagner J., Dolson E., Zaman L., &amp; Moreno M. A. (2025). Downstream: efficient cross-platform algorithms for fixed-capacity stream downsampling. arXiv preprint arXiv:2506.12975. https://doi.org/10.48550/arXiv.2506.12975</p> <pre><code>@misc{yang2025downstream,\n      doi={10.48550/arXiv.2506.12975},\n      url={https://arxiv.org/abs/2506.12975},\n      title={Downstream: efficient cross-platform algorithms for fixed-capacity stream downsampling},\n      author={Connor Yang and Joey Wagner and Emily Dolson and Luis Zaman and Matthew Andres Moreno},\n      year={2025},\n      eprint={2506.12975},\n      archivePrefix={arXiv},\n      primaryClass={cs.DS},\n}\n</code></pre> <p>And don't forget to leave a star on GitHub!</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#language-implementations","title":"Language Implementations","text":"<p>Library code for each supported programming language is implemented on a dedicated branch (e.g., Python on the <code>python</code> branch, C++ on the <code>cpp</code> branch). Implementations generally follow the following structure:</p> <ul> <li>Core algorithm variants under <code>dstream/</code> directory including:<ul> <li><code>steady_algo</code></li> <li><code>stretched_algo</code></li> <li><code>tilted_algo</code></li> <li>as needed for particular use cases, secondary algorithms and hybrid variants (e.g., <code>hybrid_0_steady_1_stretched_2_algo</code>)</li> </ul> </li> </ul> <p>Internal support code is organized into an accompanying <code>_auxlib</code> module.</p> <p>Additional implementations and outside contributions are welcome! If you create an implementation in another language, we're happy to either link to your repository or host it directly in ours on a dedicated branch. New implementations should follow the existing conventions and structure, as far as possible. Also consider implementing the project's established CLI interface for testing/debugging, which will enable out-of-the-box compatibility with our testing framework --- see the cpp or zig CI workflow for examples of how tests are run across implementations.</p> <p>If you'd like to request support for an additional programming language, please open an issue!</p> <p>Contributions to the repository are governed by our Code of Conduct, based on the Contributor Covenant, version 2.0.</p>"},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/mmore500/downstream/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>downstream could always use more documentation, whether as part of the official dstream docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/mmore500/downstream/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#deploying","title":"Deploying","text":"<p>Version bumping and deployment is triggered through the <code>release</code> GitHub action, which is dispatched manually by maintainers via the GitHub Actions web interface.</p>"},{"location":"cpp/","title":"Downstream --- C++ Implementation","text":"<p>downstream provides efficient, constant-space implementations of stream curation algorithms.</p> <ul> <li>Free software: MIT license</li> <li>Documentation: https://mmore500.github.io/downstream.</li> </ul>"},{"location":"cpp/#installation","title":"Installation","text":"<p>C++ downstream is packaged as a header-only library. It can be added to a system-wide include path, or incorporated as a git submodule in another project.</p>"},{"location":"cpp/#api-reference","title":"API Reference","text":"<p>See the Python quickstart for outline and intuition.</p> <p>Each algorithm variant is accessible through its own namespace:</p> <ul> <li>Steady: <code>downstream::dstream_steady</code></li> <li>Stretched: <code>downstream::dstream_stretched</code></li> <li>Tilted: <code>downstream::dstream_tilted</code></li> </ul> <p>See selecting a dstream algorithm for more information.</p>"},{"location":"cpp/#has_ingest_capacity","title":"<code>has_ingest_capacity</code>","text":"<p><pre><code>template &lt;std::unsigned_integral UINT = DOWNSTREAM_UINT&gt;\nbool has_ingest_capacity(const UINT S, const UINT T);\n</code></pre> Determines if there is capacity to ingest a data item at logical time T.</p> <ul> <li><code>S</code>: Buffer size (must be a power of two)</li> <li><code>T</code>: Stream position of data item (zero-indexed)</li> </ul>"},{"location":"cpp/#assign_storage_site","title":"<code>assign_storage_site</code>","text":"<p><pre><code>template &lt;std::unsigned_integral UINT = DOWNSTREAM_UINT&gt;\nstd::optional&lt;UINT&gt; assign_storage_site(const UINT S, const UINT T);\n</code></pre> Site selection algorithm for steady curation. Returns selected site or nullopt if data should be discarded.</p> <ul> <li><code>S</code>: Buffer size (must be a power of two)</li> <li><code>T</code>: Stream position of data item (zero-indexed)</li> </ul>"},{"location":"cpp/#_assign_storage_site-low-level-interface","title":"<code>_assign_storage_site</code> (low-level interface)","text":"<p><pre><code>template &lt;std::unsigned_integral UINT = DOWNSTREAM_UINT&gt;\nUINT _assign_storage_site(const UINT S, const UINT T);\n</code></pre> Returns <code>S</code> if data should be discarded.</p>"},{"location":"cpp/#citing","title":"Citing","text":"<p>If downstream contributes to a scientific publication, please cite it as</p> <p>Yang C., Wagner J., Dolson E., Zaman L., &amp; Moreno M. A. (2025). Downstream: efficient cross-platform algorithms for fixed-capacity stream downsampling. arXiv preprint arXiv:2506.12975. https://doi.org/10.48550/arXiv.2506.12975</p> <pre><code>@misc{yang2025downstream,\n      doi={10.48550/arXiv.2506.12975},\n      url={https://arxiv.org/abs/2506.12975},\n      title={Downstream: efficient cross-platform algorithms for fixed-capacity stream downsampling},\n      author={Connor Yang and Joey Wagner and Emily Dolson and Luis Zaman and Matthew Andres Moreno},\n      year={2025},\n      eprint={2506.12975},\n      archivePrefix={arXiv},\n      primaryClass={cs.DS},\n}\n</code></pre> <p>And don't forget to leave a star on GitHub!</p>"},{"location":"credits/","title":"Credits","text":""},{"location":"credits/#development-lead","title":"Development Lead","text":"<ul> <li>Matthew Andres Moreno m.more500@gmail.com</li> </ul>"},{"location":"credits/#contributors","title":"Contributors","text":"<ul> <li>Connor Yang</li> </ul>"},{"location":"csl/","title":"Downstream --- Cerebras Software Language (CSL) Implementation","text":"<p>downstream provides efficient, constant-space implementations of stream curation algorithms.</p> <ul> <li>Free software: MIT license</li> <li>Documentation: https://mmore500.github.io/downstream</li> </ul>"},{"location":"csl/#installation","title":"Installation","text":"<p>CSL downstream is packaged as a header-only library. It can be added to a system-wide include path, or incorporated as a git submodule in another project.</p> <p>Requires Cerebras SDK, available through invitation. Library CSL code targets compatibility with Cerebras SDK v1.X releases. As of October 2025, library code is tested against Cerebras SDK v1.4.0.</p>"},{"location":"csl/#api-reference","title":"API Reference","text":"<p>See the Python quickstart for outline and intuition.</p> <p>Each algorithm variant is accessible through its own module:</p> <ul> <li>Steady: <code>dstream.steady_algo</code></li> <li>Stretched: <code>dstream.stretched_algo</code></li> <li>Tilted: <code>dstream.tilted_algo</code></li> </ul> <p>See selecting a dstream algorithm for more information.</p>"},{"location":"csl/#has_ingest_capacity","title":"<code>has_ingest_capacity</code>","text":"<p><pre><code>fn has_ingest_capacity(S: u32, T: u32) bool\n</code></pre> Determines if there is capacity to ingest a data item at logical time <code>T</code>.</p> <ul> <li><code>S</code>: Buffer size (must be a power of two)</li> <li><code>T</code>: Stream position of data item (zero-indexed)</li> <li><code>T</code>: Logical time of data item</li> </ul>"},{"location":"csl/#assign_storage_site","title":"<code>assign_storage_site</code>","text":"<p><pre><code>fn assign_storage_site(S: u32, T: u32) u32\n</code></pre> Site selection algorithm for steady curation. Returns selected site or <code>S</code> if data should be discarded.</p> <ul> <li><code>S</code>: Buffer size (must be a power of two)</li> <li><code>T</code>: Stream position of data item (zero-indexed)</li> </ul>"},{"location":"csl/#citing","title":"Citing","text":"<p>If downstream contributes to a scientific publication, please cite it as</p> <p>Yang C., Wagner J., Dolson E., Zaman L., &amp; Moreno M. A. (2025). Downstream: efficient cross-platform algorithms for fixed-capacity stream downsampling. arXiv preprint arXiv:2506.12975. https://doi.org/10.48550/arXiv.2506.12975</p> <pre><code>@misc{yang2025downstream,\n      doi={10.48550/arXiv.2506.12975},\n      url={https://arxiv.org/abs/2506.12975},\n      title={Downstream: efficient cross-platform algorithms for fixed-capacity stream downsampling},\n      author={Connor Yang and Joey Wagner and Emily Dolson and Luis Zaman and Matthew Andres Moreno},\n      year={2025},\n      eprint={2506.12975},\n      archivePrefix={arXiv},\n      primaryClass={cs.DS},\n}\n</code></pre> <p>And don't forget to leave a star on GitHub!</p>"},{"location":"glossary/","title":"Glossary","text":"<ul> <li>Ingestion \u2013 reading one new item from the stream and storing or discarding it.</li> <li>Lookup \u2013 calculating the original stream index at which a stored item arrived.</li> <li>S \u2013 shorthand for surface size, i.e. the number of storage sites available.</li> <li>Site \u2013 a location in memory where one item may be stored.</li> <li>Steady \u2013 downsample distribution maintaining uniform spacing between retained items.</li> <li>Stretched \u2013 distribution that thins proportionally to depth in the stream, emphasizing older data.</li> <li>T \u2013 count of elapsed items in the stream.</li> <li>Tilted \u2013 distribution that thins proportionally to age, emphasizing recent data.</li> <li>Surface \u2013 data structure comprising dstream-structured data in fixed-capacity buffer, with counter for number of items ingested.</li> <li>hybrid algorithm \u2013 algorithm combining multiple distributions by partitioning the surface among them.</li> <li>primed algorithm \u2013 algorithm filling surface left-to-right before delegating to another algorithm.</li> </ul>"},{"location":"projects/","title":"Projects Using downstream","text":"<p>We'd love to show off your project! To be listed, make a pull request, open an issue, or send an email to m.more500@gmail.com.</p> <ul> <li> Cerebras Wafer-Scale Engine (WSE) kernel for evolution simulations and genetic algorithms </li> <li> hstrat: phylogenetic inference on distributed digital evolution populations </li> </ul>"},{"location":"publications/","title":"Publications and Presentations","text":"<p>Testing the Inference Accuracy of Accelerator-friendly Approximate Phylogeny Tracking at IEEE Symposium on Computational Intelligence in Artificial Life and Cooperative Intelligent Systems 2025 (December 5th, 2024)</p> <p>Structured Downsampling for Fast, Memory-efficient Curation of Online Data Streams for arXiv (September 10th, 2024)</p> <p>A Guide to Tracking Phylogenies in Parallel and Distributed Agent-based Evolution Models for arXiv (May 16th, 2024)</p> <p>Trackable Agent-based Evolution Models at Wafer Scale at Artificial Life 2024 (April 16th, 2024)</p> <p>Runtime phylogenetic analysis enables extreme subsampling for test-based problems at GECCO 2024 (February 2nd, 2024)</p>"},{"location":"python/","title":"Python API Reference","text":""},{"location":"python/#contents","title":"Contents","text":"<ul> <li>Algorithm API Example: <code>dstream.steady_algo</code></li> <li>Additional algorithms in <code>dstream</code></li> <li>Container API: <code>dsurf.Surface</code></li> <li>Dataframe API</li> </ul>"},{"location":"python/#algorithm-api-example-dstreamsteady_algo","title":"Algorithm API Example: <code>dstream.steady_algo</code>","text":"<p>Implements the steady downsampling algorithm, which retains items in a uniform distribution across elapsed stream history.</p> <p>Import as <pre><code>from downstream.dstream import steady_algo\n</code></pre></p> <p>Or, alternately <pre><code>from downstream import dstream\nsteady_algo = dstream.steady_algo\n</code></pre></p>"},{"location":"python/#steady_algoassign_storage_site","title":"<code>steady_algo.assign_storage_site</code>","text":""},{"location":"python/#steady_algo._steady_assign_storage_site.steady_assign_storage_site","title":"<code>steady_algo._steady_assign_storage_site.steady_assign_storage_site(S, T)</code>","text":"<p>Site selection algorithm for steady curation.</p> <p>Parameters:</p> Name Type Description Default <code>S</code> <code>int</code> <p>Buffer size. Must be a power of two.</p> required <code>T</code> <code>int</code> <p>Current logical time.</p> required <p>Returns:</p> Type Description <code>Optional[int]</code> <p>Selected site, if any.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If insufficient ingest capacity is available.</p> <p>See <code>steady_algo.has_ingest_capacity</code> for details.</p> Source code in <code>steady_algo/_steady_assign_storage_site.py</code> <pre><code>def steady_assign_storage_site(S: int, T: int) -&gt; typing.Optional[int]:\n    \"\"\"Site selection algorithm for steady curation.\n\n    Parameters\n    ----------\n    S : int\n        Buffer size. Must be a power of two.\n    T : int\n        Current logical time.\n\n    Returns\n    -------\n    typing.Optional[int]\n        Selected site, if any.\n\n    Raises\n    ------\n    ValueError\n        If insufficient ingest capacity is available.\n\n        See `steady_algo.has_ingest_capacity` for details.\n    \"\"\"\n    if not steady_has_ingest_capacity(S, T):\n        raise ValueError(f\"Insufficient ingest capacity for {S=}, {T=}\")\n\n    s = S.bit_length() - 1\n    t = T.bit_length() - s  # Current epoch (or negative)\n    h = ctz(T + 1)  # Current hanoi value\n    if h &lt; t:  # If not a top n(T) hanoi value...\n        return None  # ...discard without storing\n\n    i = T &gt;&gt; (h + 1)  # Hanoi value incidence (i.e., num seen)\n    if i == 0:  # Special case the 0th bunch\n        k_b = 0  # Bunch position\n        o = 0  # Within-bunch offset\n        w = s + 1  # Segment width\n    else:\n        j = bit_floor(i) - 1  # Num full-bunch segments\n        B = j.bit_length()  # Num full bunches\n        k_b = (1 &lt;&lt; B) * (s - B + 1)  # Bunch position\n        w = h - t + 1  # Segment width\n        assert w &gt; 0\n        o = w * (i - j - 1)  # Within-bunch offset\n\n    p = h % w  # Within-segment offset\n    return k_b + o + p  # Calculate placement site\n</code></pre>"},{"location":"python/#steady_algoassign_storage_site_batched","title":"<code>steady_algo.assign_storage_site_batched</code>","text":""},{"location":"python/#steady_algo._steady_assign_storage_site_batched.steady_assign_storage_site_batched","title":"<code>steady_algo._steady_assign_storage_site_batched.steady_assign_storage_site_batched(S, T)</code>","text":"<p>Site selection algorithm for steady curation.</p> <p>Vectorized implementation for bulk calculations.</p> <p>Parameters:</p> Name Type Description Default <code>S</code> <code>Union[ndarray, int]</code> <p>Buffer size. Must be a power of two, &lt;= 2**52.</p> required <code>T</code> <code>Union[ndarray, int]</code> <p>Current logical time. Must be &lt;= 2**52.</p> required <p>Returns:</p> Type Description <code>array</code> <p>Selected site, if any. Otherwise, S.</p> Source code in <code>steady_algo/_steady_assign_storage_site_batched.py</code> <pre><code>def steady_assign_storage_site_batched(\n    S: typing.Union[np.ndarray, int], T: typing.Union[np.ndarray, int]\n) -&gt; np.ndarray:\n    \"\"\"Site selection algorithm for steady curation.\n\n    Vectorized implementation for bulk calculations.\n\n    Parameters\n    ----------\n    S : Union[np.ndarray, int]\n        Buffer size. Must be a power of two, &lt;= 2**52.\n    T : Union[np.ndarray, int]\n        Current logical time. Must be &lt;= 2**52.\n\n    Returns\n    -------\n    np.array\n        Selected site, if any. Otherwise, S.\n    \"\"\"\n    S, T = np.atleast_1d(S).astype(np.int64), np.atleast_1d(T).astype(np.int64)\n    assert np.logical_and(S &gt; 1, np.bitwise_count(S) == 1).all()\n    # restriction &lt;= 2 ** 52 might be overly conservative\n    assert (np.maximum(S, T) &lt;= 2**52).all()\n\n    s = bitlen32(S) - 1\n    t = bitlen32(T) - s  # Current epoch (or negative)\n    h = ctz32(T + 1)  # Current hanoi value\n\n    i = T &gt;&gt; (h + 1)  # Hanoi value incidence (i.e., num seen)\n\n    j = bit_floor32(i) - 1  # Num full-bunch segments\n    B = bitlen32(j)  # Num full bunches\n    k_b = (1 &lt;&lt; B) * (s - B + 1)  # Bunch position\n    w = h - t + 1  # Segment width\n    assert np.where((h &gt;= t) &amp; (i != 0), w &gt; 0, True).all()\n    o = w * (i - j - 1)  # Within-bunch offset\n\n    # Special case the 0th bunch...\n    zeroth_bunch = i == 0\n    k_b[zeroth_bunch] = 0\n    o[zeroth_bunch] = 0\n    w[zeroth_bunch] = np.broadcast_to(s, w.shape)[zeroth_bunch] + 1\n\n    with np.errstate(divide=\"ignore\"):\n        p = h % w  # Within-segment offset\n\n    # handle discard without storing for non-top n(T) hanoi value...\n    return np.where(h &gt;= t, k_b + o + p, S)\n</code></pre>"},{"location":"python/#steady_algoget_ingest_capacity","title":"<code>steady_algo.get_ingest_capacity</code>","text":""},{"location":"python/#steady_algo._steady_get_ingest_capacity.steady_get_ingest_capacity","title":"<code>steady_algo._steady_get_ingest_capacity.steady_get_ingest_capacity(S)</code>","text":"<p>How many data item ingestions does this algorithm support?</p> <p>Returns None if the number of supported ingestions is unlimited.</p> See Also <p>has_ingest_capacity : Does this algorithm have the capacity to ingest <code>n</code> data items?</p> Source code in <code>steady_algo/_steady_get_ingest_capacity.py</code> <pre><code>def steady_get_ingest_capacity(S: int) -&gt; typing.Optional[int]:\n    \"\"\"How many data item ingestions does this algorithm support?\n\n    Returns None if the number of supported ingestions is unlimited.\n\n    See Also\n    --------\n    has_ingest_capacity : Does this algorithm have the capacity to ingest `n`\n    data items?\n    \"\"\"\n    surface_size_ok = S.bit_count() == 1 and S &gt; 1\n    return None if surface_size_ok else 0\n</code></pre>"},{"location":"python/#steady_algohas_ingest_capacity","title":"<code>steady_algo.has_ingest_capacity</code>","text":""},{"location":"python/#steady_algo._steady_has_ingest_capacity.steady_has_ingest_capacity","title":"<code>steady_algo._steady_has_ingest_capacity.steady_has_ingest_capacity(S, T)</code>","text":"<p>Does this algorithm have the capacity to ingest a data item at logical time T?</p> <p>Parameters:</p> Name Type Description Default <code>S</code> <code>int</code> <p>The number of buffer sites available.</p> required <code>T</code> <code>int</code> <p>Queried logical time.</p> required <p>Returns:</p> Type Description <code>bool</code> See Also <p>get_ingest_capacity : How many data item ingestions does this algorithm support? has_ingest_capacity_batched : Numpy-friendly implementation.</p> Source code in <code>steady_algo/_steady_has_ingest_capacity.py</code> <pre><code>def steady_has_ingest_capacity(S: int, T: int) -&gt; bool:\n    \"\"\"Does this algorithm have the capacity to ingest a data item at logical\n    time T?\n\n    Parameters\n    ----------\n    S : int\n        The number of buffer sites available.\n    T : int\n        Queried logical time.\n\n    Returns\n    -------\n    bool\n\n    See Also\n    --------\n    get_ingest_capacity : How many data item ingestions does this algorithm\n    support?\n    has_ingest_capacity_batched : Numpy-friendly implementation.\n    \"\"\"\n    assert T &gt;= 0\n    ingest_capacity = steady_get_ingest_capacity(S)\n    return ingest_capacity is None or T &lt; ingest_capacity\n</code></pre>"},{"location":"python/#steady_algohas_ingest_capacity_batched","title":"<code>steady_algo.has_ingest_capacity_batched</code>","text":""},{"location":"python/#steady_algo._steady_has_ingest_capacity_batched.steady_has_ingest_capacity_batched","title":"<code>steady_algo._steady_has_ingest_capacity_batched.steady_has_ingest_capacity_batched(S, T)</code>","text":"<p>Does this algorithm have the capacity to ingest a data item at logical time T?</p> <pre><code>Vectorized implementation for bulk calculations.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>S</code> <code>int or ndarray</code> <p>The number of buffer sites available.</p> required <code>T</code> <code>int or ndarray</code> <p>Queried logical time.</p> required <p>Returns:</p> Type Description <code>np.ndarray of bool</code> <p>True if ingest capacity is sufficient, False otherwise.</p> See Also <p>get_ingest_capacity : How many data item ingestions does this algorithm support?</p> Source code in <code>steady_algo/_steady_has_ingest_capacity_batched.py</code> <pre><code>def steady_has_ingest_capacity_batched(\n    S: typing.Union[int, np.ndarray],\n    T: typing.Union[int, np.ndarray],\n) -&gt; np.ndarray:\n    \"\"\"Does this algorithm have the capacity to ingest a data item at logical\n    time T?\n\n        Vectorized implementation for bulk calculations.\n\n    Parameters\n    ----------\n    S : int or np.ndarray\n        The number of buffer sites available.\n    T : int or np.ndarray\n        Queried logical time.\n\n    Returns\n    -------\n    np.ndarray of bool\n        True if ingest capacity is sufficient, False otherwise.\n\n    See Also\n    --------\n    get_ingest_capacity : How many data item ingestions does this algorithm\n    support?\n    \"\"\"\n    S, T = np.asarray(S), np.asarray(T)\n    assert (T &gt;= 0).all()\n\n    surface_size_ok = np.logical_and(np.bitwise_count(S) == 1, S &gt; 1)\n    return surface_size_ok + np.zeros_like(T, dtype=bool)  # Broadcast T.size\n</code></pre>"},{"location":"python/#steady_algolookup_ingest_times","title":"<code>steady_algo.lookup_ingest_times</code>","text":""},{"location":"python/#steady_algo._steady_lookup_ingest_times.steady_lookup_ingest_times","title":"<code>steady_algo._steady_lookup_ingest_times.steady_lookup_ingest_times(S, T)</code>","text":"<p>Ingest time lookup algorithm for steady curation.</p> <p>Lazy implementation.</p> <p>Parameters:</p> Name Type Description Default <code>S</code> <code>int</code> <p>Buffer size. Must be a power of two.</p> required <code>T</code> <code>int</code> <p>Current logical time.</p> required <p>Yields:</p> Type Description <code>Optional[int]</code> <p>Ingest time of stored item at buffer sites in index order.</p> Source code in <code>steady_algo/_steady_lookup_ingest_times.py</code> <pre><code>def steady_lookup_ingest_times(\n    S: int, T: int\n) -&gt; typing.Iterable[typing.Optional[int]]:\n    \"\"\"Ingest time lookup algorithm for steady curation.\n\n    Lazy implementation.\n\n    Parameters\n    ----------\n    S : int\n        Buffer size. Must be a power of two.\n    T : int\n        Current logical time.\n\n    Yields\n    ------\n    typing.Optional[int]\n        Ingest time of stored item at buffer sites in index order.\n    \"\"\"\n    assert T &gt;= 0\n    if T &lt; S:  # Patch for before buffer is filled...\n        return (v if v &lt; T else None for v in steady_lookup_impl(S, S))\n    else:  # ... assume buffer has been filled\n        return steady_lookup_impl(S, T)\n</code></pre>"},{"location":"python/#steady_algolookup_ingest_times_batched","title":"<code>steady_algo.lookup_ingest_times_batched</code>","text":""},{"location":"python/#steady_algo._steady_lookup_ingest_times_batched.steady_lookup_ingest_times_batched","title":"<code>steady_algo._steady_lookup_ingest_times_batched.steady_lookup_ingest_times_batched(S, T, parallel=True)</code>","text":"<p>Ingest time lookup algorithm for steady curation.</p> <p>Vectorized implementation for bulk calculations.</p> <p>Parameters:</p> Name Type Description Default <code>S</code> <code>int</code> <p>Buffer size. Must be a power of two.</p> required <code>T</code> <code>ndarray</code> <p>One-dimensional array of current logical times.</p> required <code>parallel</code> <code>bool</code> <p>Should numba be applied to parallelize operations?</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Ingest time of stored items at buffer sites in index order.</p> <p>Two-dimensional array. Each row corresponds to an entry in T. Contains S columns, each corresponding to buffer sites.</p> Source code in <code>steady_algo/_steady_lookup_ingest_times_batched.py</code> <pre><code>def steady_lookup_ingest_times_batched(\n    S: int,\n    T: np.ndarray,\n    parallel: bool = True,\n) -&gt; np.ndarray:\n    \"\"\"Ingest time lookup algorithm for steady curation.\n\n    Vectorized implementation for bulk calculations.\n\n    Parameters\n    ----------\n    S : int\n        Buffer size. Must be a power of two.\n    T : np.ndarray\n        One-dimensional array of current logical times.\n    parallel : bool, default True\n        Should numba be applied to parallelize operations?\n\n    Returns\n    -------\n    np.ndarray\n        Ingest time of stored items at buffer sites in index order.\n\n        Two-dimensional array. Each row corresponds to an entry in T. Contains\n        S columns, each corresponding to buffer sites.\n    \"\"\"\n    assert np.issubdtype(np.asarray(S).dtype, np.integer), S\n    assert np.issubdtype(T.dtype, np.integer), T\n\n    if (T &lt; S).any():\n        raise ValueError(\"T &lt; S not supported for batched lookup\")\n\n    return [\n        _steady_lookup_ingest_times_batched,\n        _steady_lookup_ingest_times_batched_jit,\n    ][bool(parallel)](np.int64(S), T.astype(np.int64))\n</code></pre>"},{"location":"python/#steady_algolookup_ingest_times_eager","title":"<code>steady_algo.lookup_ingest_times_eager</code>","text":""},{"location":"python/#steady_algo._steady_lookup_ingest_times_eager.steady_lookup_ingest_times_eager","title":"<code>steady_algo._steady_lookup_ingest_times_eager.steady_lookup_ingest_times_eager(S, T)</code>","text":"<p>Ingest time lookup algorithm for steady curation.</p> <p>Eager implementation.</p> <p>Parameters:</p> Name Type Description Default <code>S</code> <code>int</code> <p>Buffer size. Must be a power of two.</p> required <code>T</code> <code>int</code> <p>Current logical time.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>Ingest time of stored item at buffer sites in index order.</p> Source code in <code>steady_algo/_steady_lookup_ingest_times_eager.py</code> <pre><code>def steady_lookup_ingest_times_eager(S: int, T: int) -&gt; typing.List[int]:\n    \"\"\"Ingest time lookup algorithm for steady curation.\n\n    Eager implementation.\n\n    Parameters\n    ----------\n    S : int\n        Buffer size. Must be a power of two.\n    T : int\n        Current logical time.\n\n    Returns\n    -------\n    typing.List[int]\n        Ingest time of stored item at buffer sites in index order.\n    \"\"\"\n    if T &lt; S:\n        raise ValueError(\"T &lt; S not supported for eager lookup\")\n    return list(steady_lookup_impl(S, T))\n</code></pre>"},{"location":"python/#additional-algorithms-in-dstream","title":"Additional Algorithms in <code>dstream</code>","text":"<p>The following additional algorithms are available in the <code>dstream</code> module:</p> <ul> <li><code>circular_algo</code> implements a simple ring buffer for last n sampling.</li> <li><code>compressing_algo</code> implements steady curation via Gunther's compressing circular buffer algorithm.</li> <li><code>hybrid_0_steady_1_stretched_2_algo</code> for hybrid downsampling combining steady and stretched algorithms.</li> <li><code>hybrid_0_steady_1_stretchedxtc_2_algo</code> for hybrid downsampling combining steady and stretchedxtc algorithms.</li> <li><code>hybrid_0_steady_1_tilted_2_algo</code> for hybrid downsampling combining steady and tilted algorithms.</li> <li><code>hybrid_0_steady_1_tiltedxtc_2_algo</code> for hybrid downsampling combining steady and tiltedxtc algorithms.</li> <li><code>primed_algo_*</code> which fills surface left-to-right before delegating to another algorithm.</li> <li><code>stretched_algo</code> for stretched downsampling.</li> <li><code>stretchedxtc_algo</code> for stretched downsampling, with infinite <code>T</code> domain extension.</li> <li><code>tilted_algo</code> for tilted downsampling.</li> <li><code>tiltedxtc_algo</code> for stretched downsampling, with infinite <code>T</code> domain extension.</li> </ul> <p>These algorithms follow identical API conventions as <code>steady_algo</code>, as shown above.</p>"},{"location":"python/#container-api-dsurfsurface","title":"Container API: <code>dsurf.Surface</code>","text":"<p>Import as <pre><code>from downstream.dsurf import Surface\n</code></pre></p> <p>Or, alternately <pre><code>from downstream import dsurf\nSurface = dstream.Surface\n</code></pre></p>"},{"location":"python/#dsurfsurface","title":"<code>dsurf.Surface</code>","text":""},{"location":"python/#dsurf.Surface","title":"<code>dsurf.Surface</code>","text":"<p>               Bases: <code>Generic[_DSurfDataItem]</code></p> <p>Container orchestrating downstream curation over a fixed-size buffer.</p> Source code in <code>dsurf/_Surface.py</code> <pre><code>class Surface(typing.Generic[_DSurfDataItem]):\n    \"\"\"Container orchestrating downstream curation over a fixed-size buffer.\"\"\"\n\n    __slots__ = (\"_storage\", \"algo\", \"T\")\n\n    algo: types.ModuleType\n    _storage: typing.MutableSequence[_DSurfDataItem]\n    T: int  # current logical time\n\n    @staticmethod\n    def from_hex(\n        hex_string: str,\n        algo: types.ModuleType,\n        *,\n        S: int,\n        storage_bitoffset: typing.Optional[int] = None,\n        storage_bitwidth: int,\n        T_bitoffset: int = 0,\n        T_bitwidth: int = 32,\n    ) -&gt; \"Surface\":\n        \"\"\"\n        Deserialize a Surface object from a hex string representation.\n\n        Hex string representation needs exactly two contiguous parts:\n        1. dstream_T (which is the number of ingesgted data items), and\n        2. dstream_storage (which holds all the stored data items).\n\n        Data items are deserialized as unsigned integers.\n\n        Data in hex string representation should use big-endian byte order.\n\n        Parameters\n        ----------\n        hex_string: str\n            Hex string to be parsed, which can be uppercase or lowercase.\n        algo: module\n            Dstream algorithm to use to create the new Surface object.\n        storage_bitoffset: int, default T_bitwidth\n            Number of bits before the storage.\n        storage_bitwidth: int\n            Number of bits used for storage.\n        T_bitoffset: int, default 0\n            Number of bits before dstream_T.\n        T_bitwidth: int, default 32\n            Number of bits used to store dstream_T.\n        S: int\n            Number of buffer sites used to store data items.\n\n            Determines how many items are unpacked from storage.\n\n        See Also\n        --------\n        Surface.to_hex()\n            Serialize a Surface object into a hex string.\n        \"\"\"\n        if storage_bitoffset is None:\n            storage_bitoffset = T_bitwidth\n\n        for arg in (\n            \"storage_bitoffset\",\n            \"storage_bitwidth\",\n            \"T_bitoffset\",\n            \"T_bitwidth\",\n        ):\n            if locals()[arg] % 4:\n                msg = f\"Hex-unaligned `{arg}` not yet supported\"\n                raise NotImplementedError(msg)\n\n        storage_hexoffset = storage_bitoffset // 4\n        storage_hexwidth = storage_bitwidth // 4\n        storage_hex = hex_string[\n            storage_hexoffset : storage_hexoffset + storage_hexwidth\n        ]\n        storage = unpack_hex(storage_hex, S)\n\n        T_hexoffset = T_bitoffset // 4\n        T_hexwidth = T_bitwidth // 4\n        T_hex = hex_string[T_hexoffset : T_hexoffset + T_hexwidth]\n        T = int(T_hex, base=16)\n\n        return Surface(algo, storage, T)\n\n    def to_hex(\n        self: \"Surface\", *, item_bitwidth: int, T_bitwidth: int = 32\n    ) -&gt; str:\n        \"\"\"\n        Serialize a Surface object into a hex string representation.\n\n        Serialized data comprises two components:\n            1. dstream_T (the number of data items ingested) and\n            2. dstream_storage (binary data of data item values).\n\n        The hex layout used is:\n\n            0x...\n              ########**************************************************\n              ^                                                     ^\n              T, length = `T_bitwidth` / 4                          |\n                                                                    |\n                            storage, length = `item_bitwidth` / 4 * S\n\n        This hex string can be reconstituted by calling `Surface.from_hex()`\n        with the following parameters:\n            - `T_bitoffset` = 0\n            - `T_bitwidth` = `T_bitwidth`\n            - `storage_bitoffset` = `storage_bitoffset`\n            - `storage_bitwidth` = `self.S * item_bitwidth`\n\n        Parameters\n        ----------\n        item_bitwidth: int\n            Number of storage bits used per data item.\n        T_bitwidth: int, default 32\n            Number of bits used to store ingested items count (`self.T`).\n\n        See Also\n        --------\n        Surface.from_hex()\n            Deserialize a Surface object from a hex string.\n        \"\"\"\n        if T_bitwidth % 4:\n            raise NotImplementedError(\n                \"Hex-unaligned `T_bitwidth` not yet supported\"\n            )\n\n        if not all(isinstance(x, typing.SupportsInt) for x in self._storage):\n            raise NotImplementedError(\n                \"Non-integer hex serialization not yet implemented\"\n            )\n        if np.asarray(self._storage).min() &lt; 0:\n            raise NotImplementedError(\n                \"Negative integer hex serialization not yet implemented\"\n            )\n\n        assert self.T &gt;= 0\n        if int(self.T).bit_length() &gt; T_bitwidth:\n            raise ValueError(\n                f\"{self.T}= not representable in {T_bitwidth=} bits\",\n            )\n        T_arr = np.asarray(self.T, dtype=np.uint64)\n        T_bytes = T_arr.astype(\"&gt;u8\").tobytes()  # big-endian u64\n        T_hexwidth = T_bitwidth &gt;&gt; 2\n        T_hex = T_bytes.hex()[-T_hexwidth:]\n\n        surface_hex = pack_hex(self._storage, item_bitwidth)\n\n        return T_hex + surface_hex\n\n    def __init__(\n        self: \"Surface\",\n        algo: types.ModuleType,\n        storage: typing.Union[typing.MutableSequence[_DSurfDataItem], int],\n        T: int = 0,\n    ) -&gt; None:\n        \"\"\"Initialize a downstream Surface object, which stores hereditary\n        stratigraphic annotations using a provided algorithm.\n\n        Parameters\n        ----------\n        algo: module\n            The algorithm used by the surface to determine the placements\n            of data items. Should be one of the modules in `downstream.dstream`.\n        storage: int or MutableSequence\n            The object used to hold any ingested data. If an integer is\n            passed in, a list of length `storage` is used. Otherwise, the\n            `storage` is used directly. Random access and `__len__` must be\n            supported. For example, for efficient storage, a user may pass\n            in a NumPy array.\n        T: int, default 0\n            The initial logical time (i.e. how many items have been ingested)\n        \"\"\"\n        self.T = T\n        if isinstance(storage, int):\n            self._storage = [None] * storage\n        else:\n            self._storage = storage\n        self.algo = algo\n\n    def __repr__(self) -&gt; str:\n        return f\"Surface(algo={self.algo}, storage={self._storage})\"\n\n    def __eq__(self: \"Surface\", other: typing.Any) -&gt; bool:\n        if not isinstance(other, Surface):\n            return False\n        return (\n            self.T == other.T\n            and self.algo is other.algo\n            and [*self.lookup_zip_items()] == [*other.lookup_zip_items()]\n        )\n\n    def __iter__(\n        self: \"Surface\",\n    ) -&gt; typing.Iterator[typing.Optional[_DSurfDataItem]]:\n        return iter(self._storage)\n\n    def __getitem__(\n        self: \"Surface\", site: int\n    ) -&gt; typing.Optional[_DSurfDataItem]:\n        return self._storage[site]\n\n    def __deepcopy__(self: \"Surface\", memo: dict) -&gt; \"Surface\":\n        \"\"\"Ensure pickle compatibility when algo is a module.\"\"\"\n        new_surf = Surface(self.algo, deepcopy(self._storage, memo))\n        new_surf.T = self.T\n        return new_surf\n\n    @property\n    def S(self: \"Surface\") -&gt; int:\n        return len(self._storage)\n\n    @typing.overload\n    def lookup_zip_items(\n        self: \"Surface\",\n    ) -&gt; typing.Iterable[typing.Tuple[typing.Optional[int], _DSurfDataItem]]:\n        ...\n\n    @typing.overload\n    def lookup_zip_items(\n        self: \"Surface\", include_empty: typing.Literal[False]\n    ) -&gt; typing.Iterable[typing.Tuple[int, _DSurfDataItem]]:\n        ...\n\n    @typing.overload\n    def lookup_zip_items(\n        self: \"Surface\", include_empty: bool\n    ) -&gt; typing.Iterable[typing.Tuple[typing.Optional[int], _DSurfDataItem]]:\n        ...\n\n    def lookup_zip_items(\n        self: \"Surface\", include_empty: bool = True\n    ) -&gt; typing.Iterable[typing.Tuple[typing.Optional[int], _DSurfDataItem]]:\n        \"\"\"\n        Iterate over ingest times and values of data items in the order they\n        appear on the downstream storage, including sites not yet written to.\n        \"\"\"\n        res = zip(\n            self.lookup(include_empty=True),\n            self._storage,\n        )\n        if not include_empty:\n            return ((T, v) for T, v in res if T is not None)\n        return res\n\n    def ingest_many(\n        self: \"Surface\",\n        n_ingests: int,\n        item_getter: typing.Callable[[int], _DSurfDataItem],\n        use_relative_time: bool = False,\n    ) -&gt; None:\n        \"\"\"Ingest multiple data items.\n\n        Optimizes for the case where large amounts of data is ready to be\n        ingested, In such a scenario, we can avoid assigning multiple objects\n        to the same site, and simply iterate through sites that would be\n        updated after items were ingested.\n\n        Parameters\n        ----------\n        n_ingests : int\n            The number of data to ingest\n        item_getter : int -&gt; object\n            For a given ingest time within the n_ingests window, should\n            return the associated data item.\n        use_relative_time : bool, default False\n            Use the relative time (i.e. timesteps since current self.T)\n            instead of the absolute time as input to `item_getter`\n        \"\"\"\n\n        assert n_ingests &gt;= 0\n        if n_ingests == 0:\n            return\n\n        assert self.algo.has_ingest_capacity(self.S, self.T + n_ingests - 1)\n        for site, (T_1, T_2) in enumerate(\n            zip(\n                self.lookup(),\n                self.algo.lookup_ingest_times(self.S, self.T + n_ingests),\n            )\n        ):\n            if T_1 != T_2 and T_2 is not None:\n                self._storage[site] = item_getter(\n                    T_2 - self.T if use_relative_time else T_2\n                )\n        self.T += n_ingests\n\n    def ingest_one(\n        self: \"Surface\", item: _DSurfDataItem\n    ) -&gt; typing.Optional[int]:\n        \"\"\"Ingest data item.\n\n        Returns the storage site of the data item, or None if the data item is\n        not retained.\n        \"\"\"\n        assert self.algo.has_ingest_capacity(self.S, self.T)\n\n        site = self.algo.assign_storage_site(self.S, self.T)\n        if site is not None:\n            self._storage[site] = item\n        self.T += 1\n        return site\n\n    @typing.overload\n    def lookup(\n        self: \"Surface\",\n    ) -&gt; typing.Iterable[typing.Optional[int]]:\n        ...\n\n    @typing.overload\n    def lookup(\n        self: \"Surface\", include_empty: typing.Literal[False]\n    ) -&gt; typing.Iterable[int]:\n        ...\n\n    @typing.overload\n    def lookup(\n        self: \"Surface\", include_empty: bool\n    ) -&gt; typing.Iterable[typing.Optional[int]]:\n        ...\n\n    def lookup(\n        self: \"Surface\", include_empty: bool = True\n    ) -&gt; typing.Union[\n        typing.Iterable[typing.Optional[int]], typing.Iterable[int]\n    ]:\n        \"\"\"Iterate over data item ingest times, including null values for\n        uninitialized sites.\"\"\"\n        assert len(self._storage) == self.S\n        return (\n            T\n            for T in self.algo.lookup_ingest_times(self.S, self.T)\n            if include_empty or T is not None\n        )\n</code></pre>"},{"location":"python/#dsurf.Surface.__deepcopy__","title":"<code>__deepcopy__(memo)</code>","text":"<p>Ensure pickle compatibility when algo is a module.</p> Source code in <code>dsurf/_Surface.py</code> <pre><code>def __deepcopy__(self: \"Surface\", memo: dict) -&gt; \"Surface\":\n    \"\"\"Ensure pickle compatibility when algo is a module.\"\"\"\n    new_surf = Surface(self.algo, deepcopy(self._storage, memo))\n    new_surf.T = self.T\n    return new_surf\n</code></pre>"},{"location":"python/#dsurf.Surface.__init__","title":"<code>__init__(algo, storage, T=0)</code>","text":"<p>Initialize a downstream Surface object, which stores hereditary stratigraphic annotations using a provided algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>algo</code> <code>ModuleType</code> <p>The algorithm used by the surface to determine the placements of data items. Should be one of the modules in <code>downstream.dstream</code>.</p> required <code>storage</code> <code>Union[MutableSequence[_DSurfDataItem], int]</code> <p>The object used to hold any ingested data. If an integer is passed in, a list of length <code>storage</code> is used. Otherwise, the <code>storage</code> is used directly. Random access and <code>__len__</code> must be supported. For example, for efficient storage, a user may pass in a NumPy array.</p> required <code>T</code> <code>int</code> <p>The initial logical time (i.e. how many items have been ingested)</p> <code>0</code> Source code in <code>dsurf/_Surface.py</code> <pre><code>def __init__(\n    self: \"Surface\",\n    algo: types.ModuleType,\n    storage: typing.Union[typing.MutableSequence[_DSurfDataItem], int],\n    T: int = 0,\n) -&gt; None:\n    \"\"\"Initialize a downstream Surface object, which stores hereditary\n    stratigraphic annotations using a provided algorithm.\n\n    Parameters\n    ----------\n    algo: module\n        The algorithm used by the surface to determine the placements\n        of data items. Should be one of the modules in `downstream.dstream`.\n    storage: int or MutableSequence\n        The object used to hold any ingested data. If an integer is\n        passed in, a list of length `storage` is used. Otherwise, the\n        `storage` is used directly. Random access and `__len__` must be\n        supported. For example, for efficient storage, a user may pass\n        in a NumPy array.\n    T: int, default 0\n        The initial logical time (i.e. how many items have been ingested)\n    \"\"\"\n    self.T = T\n    if isinstance(storage, int):\n        self._storage = [None] * storage\n    else:\n        self._storage = storage\n    self.algo = algo\n</code></pre>"},{"location":"python/#dsurf.Surface.from_hex","title":"<code>from_hex(hex_string, algo, *, S, storage_bitoffset=None, storage_bitwidth, T_bitoffset=0, T_bitwidth=32)</code>  <code>staticmethod</code>","text":"<p>Deserialize a Surface object from a hex string representation.</p> <p>Hex string representation needs exactly two contiguous parts: 1. dstream_T (which is the number of ingesgted data items), and 2. dstream_storage (which holds all the stored data items).</p> <p>Data items are deserialized as unsigned integers.</p> <p>Data in hex string representation should use big-endian byte order.</p> <p>Parameters:</p> Name Type Description Default <code>hex_string</code> <code>str</code> <p>Hex string to be parsed, which can be uppercase or lowercase.</p> required <code>algo</code> <code>ModuleType</code> <p>Dstream algorithm to use to create the new Surface object.</p> required <code>storage_bitoffset</code> <code>Optional[int]</code> <p>Number of bits before the storage.</p> <code>None</code> <code>storage_bitwidth</code> <code>int</code> <p>Number of bits used for storage.</p> required <code>T_bitoffset</code> <code>int</code> <p>Number of bits before dstream_T.</p> <code>0</code> <code>T_bitwidth</code> <code>int</code> <p>Number of bits used to store dstream_T.</p> <code>32</code> <code>S</code> <code>int</code> <p>Number of buffer sites used to store data items.</p> <p>Determines how many items are unpacked from storage.</p> required See Also <p>Surface.to_hex()     Serialize a Surface object into a hex string.</p> Source code in <code>dsurf/_Surface.py</code> <pre><code>@staticmethod\ndef from_hex(\n    hex_string: str,\n    algo: types.ModuleType,\n    *,\n    S: int,\n    storage_bitoffset: typing.Optional[int] = None,\n    storage_bitwidth: int,\n    T_bitoffset: int = 0,\n    T_bitwidth: int = 32,\n) -&gt; \"Surface\":\n    \"\"\"\n    Deserialize a Surface object from a hex string representation.\n\n    Hex string representation needs exactly two contiguous parts:\n    1. dstream_T (which is the number of ingesgted data items), and\n    2. dstream_storage (which holds all the stored data items).\n\n    Data items are deserialized as unsigned integers.\n\n    Data in hex string representation should use big-endian byte order.\n\n    Parameters\n    ----------\n    hex_string: str\n        Hex string to be parsed, which can be uppercase or lowercase.\n    algo: module\n        Dstream algorithm to use to create the new Surface object.\n    storage_bitoffset: int, default T_bitwidth\n        Number of bits before the storage.\n    storage_bitwidth: int\n        Number of bits used for storage.\n    T_bitoffset: int, default 0\n        Number of bits before dstream_T.\n    T_bitwidth: int, default 32\n        Number of bits used to store dstream_T.\n    S: int\n        Number of buffer sites used to store data items.\n\n        Determines how many items are unpacked from storage.\n\n    See Also\n    --------\n    Surface.to_hex()\n        Serialize a Surface object into a hex string.\n    \"\"\"\n    if storage_bitoffset is None:\n        storage_bitoffset = T_bitwidth\n\n    for arg in (\n        \"storage_bitoffset\",\n        \"storage_bitwidth\",\n        \"T_bitoffset\",\n        \"T_bitwidth\",\n    ):\n        if locals()[arg] % 4:\n            msg = f\"Hex-unaligned `{arg}` not yet supported\"\n            raise NotImplementedError(msg)\n\n    storage_hexoffset = storage_bitoffset // 4\n    storage_hexwidth = storage_bitwidth // 4\n    storage_hex = hex_string[\n        storage_hexoffset : storage_hexoffset + storage_hexwidth\n    ]\n    storage = unpack_hex(storage_hex, S)\n\n    T_hexoffset = T_bitoffset // 4\n    T_hexwidth = T_bitwidth // 4\n    T_hex = hex_string[T_hexoffset : T_hexoffset + T_hexwidth]\n    T = int(T_hex, base=16)\n\n    return Surface(algo, storage, T)\n</code></pre>"},{"location":"python/#dsurf.Surface.ingest_many","title":"<code>ingest_many(n_ingests, item_getter, use_relative_time=False)</code>","text":"<p>Ingest multiple data items.</p> <p>Optimizes for the case where large amounts of data is ready to be ingested, In such a scenario, we can avoid assigning multiple objects to the same site, and simply iterate through sites that would be updated after items were ingested.</p> <p>Parameters:</p> Name Type Description Default <code>n_ingests</code> <code>int</code> <p>The number of data to ingest</p> required <code>item_getter</code> <code>int -&gt; object</code> <p>For a given ingest time within the n_ingests window, should return the associated data item.</p> required <code>use_relative_time</code> <code>bool</code> <p>Use the relative time (i.e. timesteps since current self.T) instead of the absolute time as input to <code>item_getter</code></p> <code>False</code> Source code in <code>dsurf/_Surface.py</code> <pre><code>def ingest_many(\n    self: \"Surface\",\n    n_ingests: int,\n    item_getter: typing.Callable[[int], _DSurfDataItem],\n    use_relative_time: bool = False,\n) -&gt; None:\n    \"\"\"Ingest multiple data items.\n\n    Optimizes for the case where large amounts of data is ready to be\n    ingested, In such a scenario, we can avoid assigning multiple objects\n    to the same site, and simply iterate through sites that would be\n    updated after items were ingested.\n\n    Parameters\n    ----------\n    n_ingests : int\n        The number of data to ingest\n    item_getter : int -&gt; object\n        For a given ingest time within the n_ingests window, should\n        return the associated data item.\n    use_relative_time : bool, default False\n        Use the relative time (i.e. timesteps since current self.T)\n        instead of the absolute time as input to `item_getter`\n    \"\"\"\n\n    assert n_ingests &gt;= 0\n    if n_ingests == 0:\n        return\n\n    assert self.algo.has_ingest_capacity(self.S, self.T + n_ingests - 1)\n    for site, (T_1, T_2) in enumerate(\n        zip(\n            self.lookup(),\n            self.algo.lookup_ingest_times(self.S, self.T + n_ingests),\n        )\n    ):\n        if T_1 != T_2 and T_2 is not None:\n            self._storage[site] = item_getter(\n                T_2 - self.T if use_relative_time else T_2\n            )\n    self.T += n_ingests\n</code></pre>"},{"location":"python/#dsurf.Surface.ingest_one","title":"<code>ingest_one(item)</code>","text":"<p>Ingest data item.</p> <p>Returns the storage site of the data item, or None if the data item is not retained.</p> Source code in <code>dsurf/_Surface.py</code> <pre><code>def ingest_one(\n    self: \"Surface\", item: _DSurfDataItem\n) -&gt; typing.Optional[int]:\n    \"\"\"Ingest data item.\n\n    Returns the storage site of the data item, or None if the data item is\n    not retained.\n    \"\"\"\n    assert self.algo.has_ingest_capacity(self.S, self.T)\n\n    site = self.algo.assign_storage_site(self.S, self.T)\n    if site is not None:\n        self._storage[site] = item\n    self.T += 1\n    return site\n</code></pre>"},{"location":"python/#dsurf.Surface.lookup","title":"<code>lookup(include_empty=True)</code>","text":"<pre><code>lookup() -&gt; typing.Iterable[typing.Optional[int]]\n</code></pre><pre><code>lookup(include_empty: typing.Literal[False]) -&gt; typing.Iterable[int]\n</code></pre><pre><code>lookup(include_empty: bool) -&gt; typing.Iterable[typing.Optional[int]]\n</code></pre> <p>Iterate over data item ingest times, including null values for uninitialized sites.</p> Source code in <code>dsurf/_Surface.py</code> <pre><code>def lookup(\n    self: \"Surface\", include_empty: bool = True\n) -&gt; typing.Union[\n    typing.Iterable[typing.Optional[int]], typing.Iterable[int]\n]:\n    \"\"\"Iterate over data item ingest times, including null values for\n    uninitialized sites.\"\"\"\n    assert len(self._storage) == self.S\n    return (\n        T\n        for T in self.algo.lookup_ingest_times(self.S, self.T)\n        if include_empty or T is not None\n    )\n</code></pre>"},{"location":"python/#dsurf.Surface.lookup_zip_items","title":"<code>lookup_zip_items(include_empty=True)</code>","text":"<pre><code>lookup_zip_items() -&gt; typing.Iterable[typing.Tuple[typing.Optional[int], _DSurfDataItem]]\n</code></pre><pre><code>lookup_zip_items(include_empty: typing.Literal[False]) -&gt; typing.Iterable[typing.Tuple[int, _DSurfDataItem]]\n</code></pre><pre><code>lookup_zip_items(include_empty: bool) -&gt; typing.Iterable[typing.Tuple[typing.Optional[int], _DSurfDataItem]]\n</code></pre> <p>Iterate over ingest times and values of data items in the order they appear on the downstream storage, including sites not yet written to.</p> Source code in <code>dsurf/_Surface.py</code> <pre><code>def lookup_zip_items(\n    self: \"Surface\", include_empty: bool = True\n) -&gt; typing.Iterable[typing.Tuple[typing.Optional[int], _DSurfDataItem]]:\n    \"\"\"\n    Iterate over ingest times and values of data items in the order they\n    appear on the downstream storage, including sites not yet written to.\n    \"\"\"\n    res = zip(\n        self.lookup(include_empty=True),\n        self._storage,\n    )\n    if not include_empty:\n        return ((T, v) for T, v in res if T is not None)\n    return res\n</code></pre>"},{"location":"python/#dsurf.Surface.to_hex","title":"<code>to_hex(*, item_bitwidth, T_bitwidth=32)</code>","text":"<p>Serialize a Surface object into a hex string representation.</p> <p>Serialized data comprises two components:     1. dstream_T (the number of data items ingested) and     2. dstream_storage (binary data of data item values).</p> <p>The hex layout used is:</p> <pre><code>0x...\n  ########**************************************************\n  ^                                                     ^\n  T, length = `T_bitwidth` / 4                          |\n                                                        |\n                storage, length = `item_bitwidth` / 4 * S\n</code></pre> <p>This hex string can be reconstituted by calling <code>Surface.from_hex()</code> with the following parameters:     - <code>T_bitoffset</code> = 0     - <code>T_bitwidth</code> = <code>T_bitwidth</code>     - <code>storage_bitoffset</code> = <code>storage_bitoffset</code>     - <code>storage_bitwidth</code> = <code>self.S * item_bitwidth</code></p> <p>Parameters:</p> Name Type Description Default <code>item_bitwidth</code> <code>int</code> <p>Number of storage bits used per data item.</p> required <code>T_bitwidth</code> <code>int</code> <p>Number of bits used to store ingested items count (<code>self.T</code>).</p> <code>32</code> See Also <p>Surface.from_hex()     Deserialize a Surface object from a hex string.</p> Source code in <code>dsurf/_Surface.py</code> <pre><code>def to_hex(\n    self: \"Surface\", *, item_bitwidth: int, T_bitwidth: int = 32\n) -&gt; str:\n    \"\"\"\n    Serialize a Surface object into a hex string representation.\n\n    Serialized data comprises two components:\n        1. dstream_T (the number of data items ingested) and\n        2. dstream_storage (binary data of data item values).\n\n    The hex layout used is:\n\n        0x...\n          ########**************************************************\n          ^                                                     ^\n          T, length = `T_bitwidth` / 4                          |\n                                                                |\n                        storage, length = `item_bitwidth` / 4 * S\n\n    This hex string can be reconstituted by calling `Surface.from_hex()`\n    with the following parameters:\n        - `T_bitoffset` = 0\n        - `T_bitwidth` = `T_bitwidth`\n        - `storage_bitoffset` = `storage_bitoffset`\n        - `storage_bitwidth` = `self.S * item_bitwidth`\n\n    Parameters\n    ----------\n    item_bitwidth: int\n        Number of storage bits used per data item.\n    T_bitwidth: int, default 32\n        Number of bits used to store ingested items count (`self.T`).\n\n    See Also\n    --------\n    Surface.from_hex()\n        Deserialize a Surface object from a hex string.\n    \"\"\"\n    if T_bitwidth % 4:\n        raise NotImplementedError(\n            \"Hex-unaligned `T_bitwidth` not yet supported\"\n        )\n\n    if not all(isinstance(x, typing.SupportsInt) for x in self._storage):\n        raise NotImplementedError(\n            \"Non-integer hex serialization not yet implemented\"\n        )\n    if np.asarray(self._storage).min() &lt; 0:\n        raise NotImplementedError(\n            \"Negative integer hex serialization not yet implemented\"\n        )\n\n    assert self.T &gt;= 0\n    if int(self.T).bit_length() &gt; T_bitwidth:\n        raise ValueError(\n            f\"{self.T}= not representable in {T_bitwidth=} bits\",\n        )\n    T_arr = np.asarray(self.T, dtype=np.uint64)\n    T_bytes = T_arr.astype(\"&gt;u8\").tobytes()  # big-endian u64\n    T_hexwidth = T_bitwidth &gt;&gt; 2\n    T_hex = T_bytes.hex()[-T_hexwidth:]\n\n    surface_hex = pack_hex(self._storage, item_bitwidth)\n\n    return T_hex + surface_hex\n</code></pre>"},{"location":"python/#dataframe-api","title":"Dataframe API","text":"<p>Import as <pre><code>from downstream import dataframe\n</code></pre></p>"},{"location":"python/#dataframeexplode_lookup_packed","title":"<code>dataframe.explode_lookup_packed</code>","text":""},{"location":"python/#dataframe._explode_lookup_packed.explode_lookup_packed","title":"<code>dataframe._explode_lookup_packed.explode_lookup_packed(df, *, calc_Tbar_argv=False, value_type, result_schema='coerce')</code>","text":"<p>Explode downstream-curated data from hexidecimal serialization of downstream buffers and counters to one-data-item-per-row, applying downstream lookup to identify origin time <code>Tbar</code> of each item.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing packed data to be exploded.</p> required <code>calc_Tbar_argv</code> <code>bool</code> <p>Include column indicating sorted order of <code>Tbar</code> values within each buffer.</p> <code>False</code> <code>value_type</code> <code>(hex, uint64, uint32, uint16, uint8)</code> <p>Type of the packed data values. Determines how the packed data is interpreted.</p> <code>'hex'</code> <code>result_schema</code> <code>(coerce, relax, shrink)</code> <p>Schema for the resulting DataFrame. Determines how the output DataFrame is structured and what types are used.</p> <code>'coerce'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with one row per data item, containing the original data and the corresponding <code>Tbar</code> value.</p> Source code in <code>dataframe/_explode_lookup_packed.py</code> <pre><code>def explode_lookup_packed(\n    df: pl.DataFrame,\n    *,\n    calc_Tbar_argv: bool = False,\n    value_type: typing.Literal[\"hex\", \"uint64\", \"uint32\", \"uint16\", \"uint8\"],\n    result_schema: typing.Literal[\"coerce\", \"relax\", \"shrink\"] = \"coerce\",\n) -&gt; pl.DataFrame:\n    \"\"\"Explode downstream-curated data from hexidecimal serialization of\n    downstream buffers and counters to one-data-item-per-row, applying\n    downstream lookup to identify origin time `Tbar` of each item.\n\n    Parameters\n    ----------\n    df : pl.DataFrame\n        DataFrame containing packed data to be exploded.\n    calc_Tbar_argv : bool, default False\n        Include column indicating sorted order of `Tbar` values within each\n        buffer.\n    value_type : {'hex', 'uint64', 'uint32', 'uint16', 'uint8'}\n        Type of the packed data values. Determines how the packed data is\n        interpreted.\n    result_schema : {'coerce', 'relax', 'shrink'}, default 'coerce'\n        Schema for the resulting DataFrame. Determines how the output DataFrame\n        is structured and what types are used.\n\n    Returns\n    -------\n    pl.DataFrame\n        DataFrame with one row per data item, containing the original data and\n        the corresponding `Tbar` value.\n    \"\"\"\n    df = unpack_data_packed(df, result_schema=result_schema)\n    return explode_lookup_unpacked(\n        df,\n        calc_Tbar_argv=calc_Tbar_argv,\n        result_schema=result_schema,\n        value_type=value_type,\n    )\n</code></pre>"},{"location":"python/#dataframeexplode_lookup_unpacked","title":"<code>dataframe.explode_lookup_unpacked</code>","text":""},{"location":"python/#dataframe._explode_lookup_unpacked.explode_lookup_unpacked","title":"<code>dataframe._explode_lookup_unpacked.explode_lookup_unpacked(df, *, calc_Tbar_argv=False, value_type, result_schema='coerce')</code>","text":"<p>Explode downstream-curated data from one-buffer-per-row (with each buffer containing multiple data items) to one-data-item-per-row, applying downstream lookup to identify origin time <code>Tbar</code> of each item.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing unpacked data with required columns, one row per dstream buffer.</p> <p>Required schema:</p> <ul> <li>'dstream_algo' : pl.Categorical<ul> <li>Name of downstream curation algorithm used</li> <li>e.g., 'dstream.steady_algo'</li> </ul> </li> <li>'dstream_S' : pl.UInt32<ul> <li>Capacity of dstream buffer, in number of data items.</li> </ul> </li> <li>'dstream_T' : pl.UInt64<ul> <li>Logical time elapsed (number of elapsed data items in stream).</li> </ul> </li> <li>'dstream_storage_hex' : pl.String<ul> <li>Raw dstream buffer binary data, containing packed data items.</li> <li>Represented as a hexadecimal string.</li> </ul> </li> </ul> <p>Optional schema:</p> <ul> <li>'downstream_version' : pl.Categorical<ul> <li>Version of downstream library used to curate data items.</li> </ul> </li> <li>'dstream_data_id' : pl.UInt64<ul> <li>Unique identifier for each data item.</li> <li>If not provided, row number will be used as identifier.</li> </ul> </li> <li>'downstream_exclude_exploded' : pl.Boolean<ul> <li>Should row be dropped after exploding unpacked data?</li> </ul> </li> <li>'downstream_validate_exploded' : pl.String, polars expression<ul> <li>Polars expression to validate exploded data.</li> </ul> </li> </ul> <p>Additional user-defined columns will be forwarded to the output DataFrame.</p> required <code>calc_Tbar_argv</code> <code>bool</code> <p>Should the algorithm calculate the argsort of ingest times for each buffer?</p> <code>False</code> <code>value_type</code> <code>(hex, uint64, uint32, uint16, uint8)</code> <p>The desired data type for the 'dstream_value' column in the output DataFrame.</p> <p>Note that 'hex' is not yet supported.</p> <code>'hex'</code> <code>result_schema</code> <code>Literal[coerce, relax, shrink]</code> <p>How should dtypes in the output DataFrame be handled? - 'coerce' : cast all columns to output schema. - 'relax' : keep all columns as-is. - 'shrink' : cast columns to smallest possible types.</p> <code>'coerce'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame with exploded data and extracted values, one row per data item from the input dstream buffers.</p> <p>Output schema:</p> <ul> <li>'dstream_data_id' : pl.UInt64<ul> <li>Identifier for dstream buffer that data item is from.</li> </ul> </li> <li>'dstream_Tbar' : pl.UInt64<ul> <li>Logical position of data item in stream (number of prior data   items).</li> </ul> </li> <li>'dstream_T' : pl.UInt64<ul> <li>Logical time elapsed (number of elapsed data items in stream).</li> </ul> </li> <li>'dstream_value' : pl.String or specified numeric type<ul> <li>Data item content, format depends on 'value_type' argument.</li> </ul> </li> <li>'dstream_value_bitwidth' : pl.UInt32<ul> <li>Size of 'dstream_value' in bits.</li> </ul> </li> </ul> <p>User-defined columns are NOT forwarded from the unpacked dataframe. To include additional columns, join the output DataFrame with the original input DataFrame.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <ul> <li>If 'dstream_value_bitwidth' is greater than 64 or equal to 2 or 3.</li> <li>If 'dstream_value_bitwidth' is not consistent across all data items.</li> <li>If 'dstream_S' is not consistent across all dstream buffers.</li> <li>If buffers aren't filled (i.e., 'dstream_T' &lt; 'dstream_S').</li> <li>If multiple dstream algorithms are present in the input DataFrame.</li> <li>If 'value_type' is 'hex'.</li> </ul> <code>ValeError</code> <p>If any of the required columns are missing from the input DataFrame.</p> See Also <p>unpack_data_packed :     Preproccessing step, converts data with downstream buffer and counter     serialized into a single hexadecimal string into input format for this     function.</p> Source code in <code>dataframe/_explode_lookup_unpacked.py</code> <pre><code>def explode_lookup_unpacked(\n    df: pl.DataFrame,\n    *,\n    calc_Tbar_argv: bool = False,\n    value_type: typing.Literal[\"hex\", \"uint64\", \"uint32\", \"uint16\", \"uint8\"],\n    result_schema: typing.Literal[\"coerce\", \"relax\", \"shrink\"] = \"coerce\",\n) -&gt; pl.DataFrame:\n    \"\"\"Explode downstream-curated data from one-buffer-per-row (with each\n    buffer containing multiple data items) to one-data-item-per-row, applying\n    downstream lookup to identify origin time `Tbar` of each item.\n\n    Parameters\n    ----------\n    df : pl.DataFrame\n        The input DataFrame containing unpacked data with required columns, one\n        row per dstream buffer.\n\n        Required schema:\n\n        - 'dstream_algo' : pl.Categorical\n            - Name of downstream curation algorithm used\n            - e.g., 'dstream.steady_algo'\n        - 'dstream_S' : pl.UInt32\n            - Capacity of dstream buffer, in number of data items.\n        - 'dstream_T' : pl.UInt64\n            - Logical time elapsed (number of elapsed data items in stream).\n        - 'dstream_storage_hex' : pl.String\n            - Raw dstream buffer binary data, containing packed data items.\n            - Represented as a hexadecimal string.\n\n        Optional schema:\n\n        - 'downstream_version' : pl.Categorical\n            - Version of downstream library used to curate data items.\n        - 'dstream_data_id' : pl.UInt64\n            - Unique identifier for each data item.\n            - If not provided, row number will be used as identifier.\n        - 'downstream_exclude_exploded' : pl.Boolean\n            - Should row be dropped after exploding unpacked data?\n        - 'downstream_validate_exploded' : pl.String, polars expression\n            - Polars expression to validate exploded data.\n\n        Additional user-defined columns will be forwarded to the output\n        DataFrame.\n\n    calc_Tbar_argv : bool, default False\n        Should the algorithm calculate the argsort of ingest times for each\n        buffer?\n\n    value_type : {'hex', 'uint64', 'uint32', 'uint16', 'uint8'}\n        The desired data type for the 'dstream_value' column in the output\n        DataFrame.\n\n        Note that 'hex' is not yet supported.\n\n    result_schema : Literal['coerce', 'relax', 'shrink'], default 'coerce'\n        How should dtypes in the output DataFrame be handled?\n        - 'coerce' : cast all columns to output schema.\n        - 'relax' : keep all columns as-is.\n        - 'shrink' : cast columns to smallest possible types.\n\n    Returns\n    -------\n    pl.DataFrame\n        A DataFrame with exploded data and extracted values, one row per data\n        item from the input dstream buffers.\n\n        Output schema:\n\n        - 'dstream_data_id' : pl.UInt64\n            - Identifier for dstream buffer that data item is from.\n        - 'dstream_Tbar' : pl.UInt64\n            - Logical position of data item in stream (number of prior data\n              items).\n        - 'dstream_T' : pl.UInt64\n            - Logical time elapsed (number of elapsed data items in stream).\n        - 'dstream_value' : pl.String or specified numeric type\n            - Data item content, format depends on 'value_type' argument.\n        - 'dstream_value_bitwidth' : pl.UInt32\n            - Size of 'dstream_value' in bits.\n\n        User-defined columns are NOT forwarded from the unpacked dataframe. To\n        include additional columns, join the output DataFrame with the original\n        input DataFrame.\n\n    Raises\n    ------\n    NotImplementedError\n        - If 'dstream_value_bitwidth' is greater than 64 or equal to 2 or 3.\n        - If 'dstream_value_bitwidth' is not consistent across all data items.\n        - If 'dstream_S' is not consistent across all dstream buffers.\n        - If buffers aren't filled (i.e., 'dstream_T' &lt; 'dstream_S').\n        - If multiple dstream algorithms are present in the input DataFrame.\n        - If 'value_type' is 'hex'.\n    ValeError\n        If any of the required columns are missing from the input DataFrame.\n\n    See Also\n    --------\n    unpack_data_packed :\n        Preproccessing step, converts data with downstream buffer and counter\n        serialized into a single hexadecimal string into input format for this\n        function.\n    \"\"\"\n    _check_df(df)\n    value_dtype = _get_value_dtype(value_type)\n\n    if df.lazy().limit(1).collect().is_empty():\n        return _make_empty(value_dtype)\n\n    dstream_S = df.lazy().select(\"dstream_S\").limit(1).collect().item()\n    dstream_algo = df.lazy().select(\"dstream_algo\").limit(1).collect().item()\n    dstream_algo = eval(dstream_algo, {\"dstream\": dstream})\n    num_records = df.lazy().select(pl.len()).collect().item()\n    num_items = num_records * dstream_S\n\n    logging.info(\"begin explode_lookup_unpacked\")\n    logging.info(\" - prepping data...\")\n    df = _prep_data(df, num_records=num_records, dstream_S=dstream_S)\n    _check_bitwidths(df)\n\n    logging.info(\" - exploding dataframe...\")\n    df_long = df.drop(\"dstream_storage_hex\").select(\n        pl.all().gather(np.repeat(np.arange(num_records), dstream_S)),\n    )\n\n    logging.info(\" - unpacking hex strings...\")\n    df_long = _unpack_hex_strings(\n        df, df_long, num_items=num_items, value_dtype=value_dtype\n    )\n\n    logging.info(\" - looking up ingest times...\")\n    df_long = _lookup_ingest_times(\n        df,\n        df_long,\n        calc_Tbar_argv=calc_Tbar_argv,\n        lookup_op=dstream_algo.lookup_ingest_times_batched,\n        dstream_S=dstream_S,\n    )\n\n    if __debug__:\n        logging.info(\" - checking lookup bounds...\")\n        _check_lookup_bounds(df_long)\n\n    if \"downstream_validate_exploded\" in df_long:\n        logging.info(\" - evaluating `downstream_validate_unpacked` exprs...\")\n        df_long = _perform_validation(df_long)\n\n    if \"downstream_exclude_exploded\" in df_long:\n        logging.info(\" - dropping excluded rows...\")\n        df_long = _drop_excluded_rows(df_long)\n\n    logging.info(\" - finalizing result schema\")\n    df_long = _finalize_result_schema(\n        df_long, result_schema=result_schema, value_dtype=value_dtype\n    )\n\n    logging.info(\"explode_lookup_unpacked complete\")\n    return df_long\n</code></pre>"},{"location":"python/#dataframeunpack_data_packed","title":"<code>dataframe.unpack_data_packed</code>","text":""},{"location":"python/#dataframe._unpack_data_packed.unpack_data_packed","title":"<code>dataframe._unpack_data_packed.unpack_data_packed(df, *, result_schema='coerce')</code>","text":"<p>Unpack data with dstream buffer and counter serialized into a single hexadecimal data field.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing packed data with required columns, one row per dstream buffer.</p> <p>Required schema:</p> <ul> <li>'data_hex' : pl.String<ul> <li>Raw binary data, with serialized dstream buffer and counter.</li> <li>Represented as a hexadecimal string.</li> </ul> </li> <li>'dstream_algo' : pl.Categorical<ul> <li>Name of downstream curation algorithm used.</li> <li>e.g., 'dstream.steady_algo'</li> </ul> </li> <li>'dstream_storage_bitoffset' : pl.UInt64<ul> <li>Position of dstream buffer field in 'data_hex'.</li> </ul> </li> <li>'dstream_storage_bitwidth' : pl.UInt64<ul> <li>Size of dstream buffer field in 'data_hex'.</li> </ul> </li> <li>'dstream_T_bitoffset' : pl.UInt64<ul> <li>Position of dstream counter field in 'data_hex'.</li> </ul> </li> <li>'dstream_T_bitwidth' : pl.UInt64<ul> <li>Size of dstream counter field in 'data_hex'.</li> </ul> </li> <li>'dstream_S' : pl.UInt32<ul> <li>Capacity of dstream buffer, in number of data items.</li> </ul> </li> </ul> <p>Optional schema:</p> <ul> <li>'downstream_version' : pl.Categorical<ul> <li>Version of downstream library used to curate data items.</li> </ul> </li> <li>'downstream_exclude_exploded' : pl.Boolean<ul> <li>Should row be dropped after exploding unpacked data?</li> </ul> </li> <li>'downstream_exclude_unpacked' : pl.Boolean<ul> <li>Should row be dropped after unpacking packed data?</li> </ul> </li> <li>'downstream_validate_exploded' : pl.String, polars expression<ul> <li>Polars expression to validate exploded data.</li> </ul> </li> <li>'downstream_validate_unpacked' : pl.String, polars expression<ul> <li>Polars expression to validate unpacked data.</li> </ul> </li> </ul> required <code>result_schema</code> <code>Literal[coerce, relax, shrink]</code> <p>How should dtypes in the output DataFrame be handled?</p> <ul> <li>'coerce' : cast all columns to output schema.</li> <li>'relax' : keep all columns as-is.</li> <li>'shrink' : cast columns to smallest possible types.</li> </ul> <code>'coerce'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Processed DataFrame with unpacked and decoded data fields, one row per dstream buffer</p> <p>Output schema:     - 'dstream_algo' : pl.Categorical         - Name of downstream curation algorithm used.         - e.g., 'dstream.steady_algo'     - 'dstream_data_id' : pl.UInt64         - Row index identifier for dstream buffer.     - 'dstream_S' : pl.UInt32         - Capacity of dstream buffer, in number of data items.     - 'dstream_T' : pl.UInt64         - Logical time elapsed (number of elapsed data items in stream).     - 'dstream_storage_hex' : pl.String         - Raw dstream buffer binary data, containing packed data items.         - Represented as a hexadecimal string.</p> <p>User-defined columns and 'downstream_version' will be forwarded from the input DataFrame.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If any of the bit offset or bit width columns are not hex-aligned (i.e., not multiples of 4 bits).</p> <code>ValueError</code> <p>If any of the required columns are missing from the input DataFrame.</p> See Also <p>downstream.dataframe.explode_lookup_unpacked :     Explodes unpacked buffers into individual constituent data items.</p> Source code in <code>dataframe/_unpack_data_packed.py</code> <pre><code>def unpack_data_packed(\n    df: pl.DataFrame,\n    *,\n    result_schema: typing.Literal[\"coerce\", \"relax\", \"shrink\"] = \"coerce\",\n) -&gt; pl.DataFrame:\n    \"\"\"Unpack data with dstream buffer and counter serialized into a single\n    hexadecimal data field.\n\n    Parameters\n    ----------\n    df : pl.DataFrame\n        The input DataFrame containing packed data with required columns, one\n        row per dstream buffer.\n\n        Required schema:\n\n        - 'data_hex' : pl.String\n            - Raw binary data, with serialized dstream buffer and counter.\n            - Represented as a hexadecimal string.\n        - 'dstream_algo' : pl.Categorical\n            - Name of downstream curation algorithm used.\n            - e.g., 'dstream.steady_algo'\n        - 'dstream_storage_bitoffset' : pl.UInt64\n            - Position of dstream buffer field in 'data_hex'.\n        - 'dstream_storage_bitwidth' : pl.UInt64\n            - Size of dstream buffer field in 'data_hex'.\n        - 'dstream_T_bitoffset' : pl.UInt64\n            - Position of dstream counter field in 'data_hex'.\n        - 'dstream_T_bitwidth' : pl.UInt64\n            - Size of dstream counter field in 'data_hex'.\n        - 'dstream_S' : pl.UInt32\n            - Capacity of dstream buffer, in number of data items.\n\n        Optional schema:\n\n        - 'downstream_version' : pl.Categorical\n            - Version of downstream library used to curate data items.\n        - 'downstream_exclude_exploded' : pl.Boolean\n            - Should row be dropped after exploding unpacked data?\n        - 'downstream_exclude_unpacked' : pl.Boolean\n            - Should row be dropped after unpacking packed data?\n        - 'downstream_validate_exploded' : pl.String, polars expression\n            - Polars expression to validate exploded data.\n        - 'downstream_validate_unpacked' : pl.String, polars expression\n            - Polars expression to validate unpacked data.\n\n    result_schema : Literal['coerce', 'relax', 'shrink'], default 'coerce'\n        How should dtypes in the output DataFrame be handled?\n\n        - 'coerce' : cast all columns to output schema.\n        - 'relax' : keep all columns as-is.\n        - 'shrink' : cast columns to smallest possible types.\n\n    Returns\n    -------\n    pl.DataFrame\n        Processed DataFrame with unpacked and decoded data fields, one row per\n        dstream buffer\n\n        Output schema:\n            - 'dstream_algo' : pl.Categorical\n                - Name of downstream curation algorithm used.\n                - e.g., 'dstream.steady_algo'\n            - 'dstream_data_id' : pl.UInt64\n                - Row index identifier for dstream buffer.\n            - 'dstream_S' : pl.UInt32\n                - Capacity of dstream buffer, in number of data items.\n            - 'dstream_T' : pl.UInt64\n                - Logical time elapsed (number of elapsed data items in stream).\n            - 'dstream_storage_hex' : pl.String\n                - Raw dstream buffer binary data, containing packed data items.\n                - Represented as a hexadecimal string.\n\n        User-defined columns and 'downstream_version' will be forwarded from\n        the input DataFrame.\n\n    Raises\n    ------\n    NotImplementedError\n        If any of the bit offset or bit width columns are not hex-aligned\n        (i.e., not multiples of 4 bits).\n    ValueError\n        If any of the required columns are missing from the input DataFrame.\n\n\n    See Also\n    --------\n    downstream.dataframe.explode_lookup_unpacked :\n        Explodes unpacked buffers into individual constituent data items.\n    \"\"\"\n    logging.info(\"begin explode_lookup_unpacked\")\n    logging.info(\" - prepping data...\")\n\n    _check_df(df)\n    if df.lazy().limit(1).collect().is_empty():\n        return _make_empty()\n\n    df = df.cast({\"data_hex\": pl.String, \"dstream_algo\": pl.Categorical})\n\n    logging.info(\" - calculating offsets...\")\n    df = _calculate_offsets(df)\n\n    if \"dstream_data_id\" not in df.lazy().collect_schema().names():\n        df = df.with_row_index(\"dstream_data_id\")\n\n    logging.info(\" - extracting T and storage_hex from data_hex...\")\n    df = _extract_from_data_hex(df)\n\n    if \"downstream_validate_unpacked\" in df:\n        logging.info(\" - evaluating `downstream_validate_unpacked` exprs...\")\n        df = _perform_validations(df)\n\n    if \"downstream_exclude_unpacked\" in df:\n        logging.info(\" - dropping excluded rows...\")\n        df = _drop_excluded_rows(df)\n\n    logging.info(\" - finalizing result schema...\")\n    df = _finalize_result_schema(df, result_schema)\n\n    logging.info(\"unpack_data_packed complete\")\n    return df\n</code></pre>"},{"location":"python/#command-line-interface","title":"Command Line Interface","text":"<p>For information on available command line interface (CLI) commands <pre><code>python3 -m downstream --help\n</code></pre></p>"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#ring-buffer-generalization-intuition","title":"Ring Buffer Generalization Intuition","text":"<p>Data streams consist of a strictly ordered sequence of read once inputs. They often exceed available memory capacity.</p> <p>Traditional approaches like circular ring buffers keep only the most recent data points. They discard older information.</p> <p></p> <p>In contrast, downstream maintains representative records of stream history using three algorithms. The steady algorithm creates evenly spaced snapshots across the entire history. The stretched algorithm preserves important older data points. The tilted algorithm prioritizes recent information.  We provide a more detailed description of available algorithms in Selecting a downstream Algorithm. That page offers guidance on picking one for your use case.</p>"},{"location":"quickstart/#installing","title":"Installing","text":"<p>To install from PyPi with pip, run</p> <pre><code>python3 -m pip install downstream\n</code></pre> <p>Or optionally, to install with JIT</p> <pre><code>python3 -m pip install \"downstream[jit]\"\n</code></pre> <p>A containerized release of downstream is available via https://ghcr.io.</p> <pre><code>singularity exec docker://ghcr.io/mmore500/downstream python3 -m downstream --help\n</code></pre> <p>downstream is also available in C++, Rust, Zig, and CSL.</p> <p>Installation instructions are available on each of their respective pages.</p>"},{"location":"quickstart/#container-interface","title":"Container Interface","text":"<p>The Python <code>downstream</code> library provides an object-oriented interface for managing downstream data.</p> <pre><code>from downstream.dsurf import Surface\nfrom downstream.dstream import steady_algo\n\nS = 8  # Buffer size, must be a power of 2\nsurface = Surface(steady_algo, S)\nassert surface.T == 0\nassert [*surface] == [None] * surface.S\nassert [*surface.lookup()] == [None] * surface.S\n\nfor T in range(100):\n    site = surface.ingest_one(T)\n    if site is not None:\n        assert surface[site] == T\n    assert [*surface] == [*surface.lookup()]\n\nprint([*surface.lookup_zip_items()])\n</code></pre>"},{"location":"quickstart/#buffer-interface","title":"Buffer Interface","text":"<p>For other languages or lower-level use cases, the <code>downstream</code> library provides a direct buffer interface, where responsbility for data storage is delegated to the user.</p> <ul> <li> <p>Site assignment maps data item index <code>T</code> to a storage location using either:</p> <ul> <li><code>assign_storage_site</code>: For processing single data points</li> <li><code>assign_storage_site_batched</code>: For efficient bulk processing of multiple data points</li> </ul> </li> </ul> <pre><code>from downstream import dstream\n\n# Initialize a buffer with size 8 (must be a power of 2)\nS = 8\n\n# Process a stream of data items\nfor T in range(20):\n    # Determine site based on buffer size and index T\n    site = dstream.steady_algo.assign_storage_site(S, T)\n\n    if site is not None:\n        # Store data at the selected site\n        print(f\"Data point {T} stored at position {site}\")\n</code></pre> <p>For high-level interfaces, a null value (e.g., <code>None</code> in Python) indicates that the data item should be discarded. For low-level interfaces, a site index of <code>S</code> indicates that the data item should be discarded.</p> <p>For hstrat users: to update a genome annotation, you can store a random differentia or make a random choice whether to toggle single-bit differentia with each generation elapsed.</p>"},{"location":"quickstart/#lookup","title":"Lookup","text":"<p>The Python implementation provides <code>lookup_ingest_times</code> to recover the stream index of values stored in a buffer. For high throughput workloads, use <code>lookup_ingest_times_batched</code> which applies NumPy vectorization and Numba parallelization for speed.</p> <p>Most workflows serialize buffer contents and run lookups in bulk rather than online, as described next.</p>"},{"location":"quickstart/#dataframe-based-postprocessing","title":"DataFrame-based Postprocessing","text":"<p>After processing a stream, you can serialize the buffer contents into a tabular data file format like CSV or Parquet. To run a lookup on serialized data, you can use the <code>downstream.dataframe.explode_lookup_packed_uint</code> command.</p> <p><code>input.csv</code>: <pre><code>dstream_algo,downstream_version,data_hex,dstream_storage_bitoffset,dstream_storage_bitwidth,dstream_T_bitoffset,dstream_T_bitwidth,dstream_S\ndstream.steady_algo,1.0.1,080001030702050406,8,64,0,8,8\ndstream.steady_algo,1.0.1,0b0001030702050906,8,64,0,8,8\n</code></pre></p> <pre><code>ls input.csv | python3 -m downstream.dataframe.explode_lookup_packed_uint output.csv\n</code></pre> <p><code>output.csv</code> <pre><code>dstream_data_id,dstream_T,dstream_value_bitwidth,dstream_value,dstream_Tbar\n0,8,8,0,0\n0,8,8,1,1\n0,8,8,3,3\n0,8,8,7,7\n0,8,8,2,2\n0,8,8,5,5\n0,8,8,4,4\n0,8,8,6,6\n1,11,8,0,0\n1,11,8,1,1\n1,11,8,3,3\n1,11,8,7,7\n1,11,8,2,2\n1,11,8,5,5\n1,11,8,9,9\n1,11,8,6,6\n</code></pre></p> <p>A library-based Python API for DataFrame operations is also provided, described here. Full information on DataFrame column conventions is provided in docstrings for corresponding library functions.</p> <p>A reference implementation of hex serialization can be found in source code for <code>dsurf.Surface.to_hex</code>.</p>"},{"location":"rust/","title":"Downstream --- Rust Implementation","text":"<p>downstream provides efficient, constant-space implementations of stream curation algorithms.</p> <ul> <li>Free software: MIT license</li> <li>Documentation: https://mmore500.github.io/downstream</li> </ul>"},{"location":"rust/#installation","title":"Installation","text":"<p>Add downstream as a dependency in your <code>Cargo.toml</code>:</p> <pre><code>[dependencies]\ndownstream = \"&gt;=0.0.0\"\n</code></pre>"},{"location":"rust/#api-reference","title":"API Reference","text":"<p>See the Python quickstart for outline and intuition.</p> <p>Each algorithm variant is accessible through the <code>downstream::dstream</code> module:</p> <ul> <li>Steady: <code>downstream::dstream::SteadyAlgo</code></li> <li>Stretched: <code>downstream::dstream::StretchedAlgo</code></li> <li>Tilted: <code>downstream::dstream::TiltedAlgo</code></li> </ul> <p>See selecting a dstream algorithm for more information.</p>"},{"location":"rust/#has_ingest_capacity","title":"<code>has_ingest_capacity</code>","text":"<p><pre><code>pub fn has_ingest_capacity&lt;Uint: downstream::_auxlib::UnsignedTrait&gt;(S: Uint, T: Uint) -&gt; bool\n</code></pre> Determines if there is capacity to ingest a data item at logical time <code>T</code>.</p> <ul> <li><code>Uint</code>: Integer type (e.g., <code>u32</code>)</li> <li><code>S</code>: Buffer size (must be a power of two)</li> <li><code>T</code>: Stream position of data item (zero-indexed)</li> </ul>"},{"location":"rust/#assign_storage_site","title":"<code>assign_storage_site</code>","text":"<p><pre><code>pub fn assign_storage_site&lt;Uint: downstream::_auxlib::UnsignedTrait&gt;(S: Uint, T: Uint) -&gt; Option&lt;Uint&gt;\n</code></pre> Site selection algorithm for steady curation. Returns selected site or <code>None</code> if data should be discarded.</p> <ul> <li><code>Uint</code>: Integer type (e.g., <code>u32</code>)</li> <li><code>S</code>: Buffer size (must be a power of two)</li> <li><code>T</code>: Stream position of data item (zero-indexed)</li> </ul>"},{"location":"rust/#_assign_storage_site-low-level-interface","title":"<code>_assign_storage_site</code> (low-level interface)","text":"<p><pre><code>pub fn _assign_storage_site&lt;Uint: downstream::_auxlib::UnsignedTrait&gt;(S: Uint, T: Uint) -&gt; Uint\n</code></pre> Returns <code>S</code> if data should be discarded.</p>"},{"location":"rust/#citing","title":"Citing","text":"<p>If downstream contributes to a scientific publication, please cite it as</p> <p>Yang C., Wagner J., Dolson E., Zaman L., &amp; Moreno M. A. (2025). Downstream: efficient cross-platform algorithms for fixed-capacity stream downsampling. arXiv preprint arXiv:2506.12975. https://doi.org/10.48550/arXiv.2506.12975</p> <pre><code>@misc{yang2025downstream,\n      doi={10.48550/arXiv.2506.12975},\n      url={https://arxiv.org/abs/2506.12975},\n      title={Downstream: efficient cross-platform algorithms for fixed-capacity stream downsampling},\n      author={Connor Yang and Joey Wagner and Emily Dolson and Luis Zaman and Matthew Andres Moreno},\n      year={2025},\n      eprint={2506.12975},\n      archivePrefix={arXiv},\n      primaryClass={cs.DS},\n}\n</code></pre> <p>And don't forget to leave a star on GitHub!</p>"},{"location":"zig/","title":"Downstream --- Zig Implementation","text":"<p>downstream provides efficient, constant-space implementations of stream curation algorithms.</p> <ul> <li>Free software: MIT license</li> <li>Documentation: https://mmore500.github.io/downstream</li> </ul>"},{"location":"zig/#api-reference","title":"API Reference","text":"<p>See the Python quickstart for outline and intuition.</p> <p>Each algorithm variant is accessible through its own module:</p> <ul> <li>Steady: <code>downstream.dstream.steady_algo</code></li> <li>Stretched: <code>downstream.dstream.stretched_algo</code></li> <li>Tilted: <code>downstream.dstream.tilted_algo</code></li> </ul> <p>See selecting a dstream algorithm for more information.</p>"},{"location":"zig/#has_ingest_capacity","title":"<code>has_ingest_capacity</code>","text":"<p><pre><code>pub fn has_ingest_capacity(comptime u: type, S: u, T: u) bool\n</code></pre> Determines if there is capacity to ingest a data item at logical time <code>T</code>.</p> <ul> <li><code>u</code>: Integer type (e.g., <code>u32</code>)</li> <li><code>S</code>: Buffer size (must be a power of two)</li> <li><code>T</code>: Stream position of data item (zero-indexed)</li> </ul>"},{"location":"zig/#assign_storage_site","title":"<code>assign_storage_site</code>","text":"<p><pre><code>pub fn assign_storage_site(comptime u: type, S: u, T: u) u\n</code></pre> Site selection algorithm for steady curation. Returns selected site or <code>S</code> if data should be discarded.</p> <ul> <li><code>S</code>: Buffer size (must be a power of two)</li> <li><code>T</code>: Stream position of data item (zero-indexed)</li> </ul>"},{"location":"zig/#citing","title":"Citing","text":"<p>If downstream contributes to a scientific publication, please cite it as</p> <p>Yang C., Wagner J., Dolson E., Zaman L., &amp; Moreno M. A. (2025). Downstream: efficient cross-platform algorithms for fixed-capacity stream downsampling. arXiv preprint arXiv:2506.12975. https://doi.org/10.48550/arXiv.2506.12975</p> <pre><code>@misc{yang2025downstream,\n      doi={10.48550/arXiv.2506.12975},\n      url={https://arxiv.org/abs/2506.12975},\n      title={Downstream: efficient cross-platform algorithms for fixed-capacity stream downsampling},\n      author={Connor Yang and Joey Wagner and Emily Dolson and Luis Zaman and Matthew Andres Moreno},\n      year={2025},\n      eprint={2506.12975},\n      archivePrefix={arXiv},\n      primaryClass={cs.DS},\n}\n</code></pre> <p>And don't forget to leave a star on GitHub!</p>"}]}